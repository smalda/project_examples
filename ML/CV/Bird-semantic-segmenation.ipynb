{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12ac6f5",
   "metadata": {},
   "source": [
    "# Семантическая сегментация птиц\n",
    "![birds_semantic_seg.jpg](birds_semantic_seg.jpg)\n",
    "\n",
    "Необходимо реализовать бинарную сегментацию изображений птиц."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283327e",
   "metadata": {},
   "source": [
    "Классификация пикселей осуществляется на базе [Lite R-ASPP](https://pytorch.org/vision/main/models/generated/torchvision.models.segmentation.lraspp_mobilenet_v3_large.html). Исходная модель тренируется предсказывать 21 класс из датасета COCO вместе с фоном, нам нужен только `classes[3]=='bird'`, поэтому \n",
    "от свёрток в голове `low_classifier` и `high_classifier` оставляем только веса соответствующие этому классу.  \n",
    "При этом незамороженную часть скелета `ResNet` тренируем с `lr` меньше, чем у классификатора, чтобы не потерять выученные репрезентации и сконцентрироваться на `classifier`.  \n",
    "\n",
    "\n",
    "Ошибка, оптимизируемая сетью - $\\alpha\\mathrm{DiceLoss}*\\mathrm{size}(batch) + (1-\\alpha)\\mathrm{CE}, \\alpha=0.1$, позволяющая совместить эффективность $\\mathrm{DiceLoss}$ при дисбалансе классов (в бинарной маске) и хорошие качества сходимости $\\mathrm{CE}$ (не идеально, но для учебного примера подходит)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887b97a",
   "metadata": {},
   "source": [
    "Структура хранения данных - в папке `image_folder` находятся изображения птиц, разбитые по папкам-классам, в `gt_folder` лежат ground truth маски сегментации:  \n",
    "<ul>\n",
    "    <li> image_folder </li>\n",
    "    <ul>\n",
    "        <li> bird_type_1 </li>\n",
    "        <ul>\n",
    "            <li> image_1.jpg </li>\n",
    "            <li> image_2.jpg </li>\n",
    "            <li> ... </li>\n",
    "        </ul>\n",
    "        <li> bird_type_2 </li>\n",
    "        <li> ... </li>\n",
    "    </ul>\n",
    "    <li> gt_folder </li>\n",
    "    <ul>\n",
    "        <li> bird_type_1 </li>\n",
    "        <ul>\n",
    "            <li> image_1.png </li>\n",
    "            <li> image_2.png </li>\n",
    "            <li> ... </li>\n",
    "        </ul>\n",
    "        <li> bird_type_2 </li>\n",
    "        <li> ... </li>\n",
    "    </ul>\n",
    "</ul>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab61f123",
   "metadata": {},
   "source": [
    "Для обучения делается `stratified split` по классам птиц, чтобы не дать сети переобучиться на маджорити классах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db998b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision.models.segmentation import lraspp_mobilenet_v3_large\n",
    "\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS_NUM = 40\n",
    "BATCH_SIZE = 64\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=1)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=1) +\\\n",
    "                                                 target.sum(dim=2).sum(dim=1) + smooth)))\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "MEAN=[0.485, 0.456, 0.406]\n",
    "STD=[0.229, 0.224, 0.225]\n",
    "\n",
    "DEFAULT_TRAIN_TRANSFORM = A.Compose(\n",
    "    [\n",
    "        A.Resize(520, 520),\n",
    "        A.RandomSizedCrop([460,500], 520, 520),\n",
    "        A.Rotate(20),\n",
    "        A.Normalize(mean=MEAN, std=STD),\n",
    "        A.ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "DEFAULT_VAL_TRANSFORM = A.Compose(\n",
    "    [\n",
    "        A.Resize(520, 520),\n",
    "        A.Normalize(mean=MEAN, std=STD),\n",
    "        A.ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# torch Dataset for train and validation phases\n",
    "# samples = all filenames ([..., bird_i_image_j.jpg, ...])\n",
    "# transform = albumentations transformations to be applied\n",
    "class BirdDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples, image_folder, gt_folder, transform) -> None:\n",
    "        super(BirdDataset, self).__init__()\n",
    "        self.samples = samples\n",
    "        self.image_folder = image_folder\n",
    "        self.gt_folder = gt_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.samples[index]\n",
    "\n",
    "        # [:,:,::-1] to get RGB format\n",
    "        img = cv2.imread(os.path.join(self.image_folder, img_path))[:,:,::-1]\n",
    "        img = img.astype(np.float32)\n",
    "\n",
    "        label = cv2.imread(os.path.join(self.gt_folder, f'{img_path[:-3]}png'))\n",
    "        # we don't need to predict the same values\n",
    "        if len(label.shape) == 3:\n",
    "            label = label[:, :, 0]\n",
    "        # for simplicity let's predict a binary mask\n",
    "        label = (label > 127).astype(float)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            output = self.transform(image=img, mask=label)\n",
    "            img = output['image']\n",
    "            label = output['mask']\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "# model outputs logits for each pixel\n",
    "class BirdSegmenter(pl.LightningModule):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = lraspp_mobilenet_v3_large(pretrained=pretrained, pretrained_backbone=pretrained)\n",
    "        \n",
    "        # we preserve only the weights associated with classes[3]=='bird'\n",
    "        new_low_classifier = nn.Conv2d(40, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "        new_low_classifier.weight = nn.Parameter(self.model.classifier.low_classifier.weight.data[[0, 3]])\n",
    "        new_low_classifier.bias = nn.Parameter(self.model.classifier.low_classifier.bias.data[[0, 3]])\n",
    "        self.model.classifier.low_classifier = new_low_classifier\n",
    "\n",
    "        new_high_classifier = nn.Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "        new_high_classifier.weight = nn.Parameter(self.model.classifier.high_classifier.weight.data[[0, 3]])\n",
    "        new_high_classifier.bias = nn.Parameter(self.model.classifier.high_classifier.bias.data[[0, 3]])\n",
    "        self.model.classifier.high_classifier = new_high_classifier\n",
    "        \n",
    "        # we'll train only 2 InvertedResidual with Conv2dNormActivation at the end of the backbone and LRASPPHead\n",
    "        layers_to_freeze = list(self.model.backbone.children())[:-3]\n",
    "        for layer in layers_to_freeze:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # CrossEntropy weight in total loss\n",
    "        self.ce_weight = 0.9\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # REQUIRED\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)['out']\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    # REQUIRED\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.to(torch.long)\n",
    "\n",
    "        y_logit = self(x)\n",
    "        ce = self.criterion(y_logit, y)\n",
    "        \n",
    "        pred = F.softmax(y_logit, dim=1)[:,1,:,:]\n",
    "        dice = dice_loss(pred, y)\n",
    "\n",
    "        loss = ce * self.ce_weight + dice * (1 - self.ce_weight) * y.size(0)\n",
    "\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    # REQUIRED\n",
    "    def configure_optimizers(self):\n",
    "        # we'll teach  ResNet backbone parameters with lower lr to avoid losing efficiency of representations\n",
    "        # and focus on classifier learning\n",
    "        backbone_layers_to_finetune = list(self.model.backbone.children())[-3:]\n",
    "        backbone_parameters_to_finetune = []\n",
    "        for layer in backbone_layers_to_finetune:\n",
    "            for p in layer.parameters():\n",
    "                backbone_parameters_to_finetune.append(p)\n",
    "        grouped_parameters = [\n",
    "            {'params': backbone_parameters_to_finetune, 'lr': 1e-5},\n",
    "            {'params': self.model.classifier.parameters(), 'lr': 1e-3},\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(grouped_parameters)\n",
    "        \n",
    "        # reduce lr when learning stagnates to better find minima\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                                  mode='min', \n",
    "                                                                  factor=0.1, \n",
    "                                                                  patience=10, \n",
    "                                                                  verbose=True)\n",
    "        \n",
    "        lr_dict = {\n",
    "            \"scheduler\": lr_scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [lr_dict]\n",
    "    \n",
    "    # OPTIONAL\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"the full validation loop\"\"\"\n",
    "        x, y = batch\n",
    "        y = y.to(torch.long)\n",
    "\n",
    "        y_logit = self(x)\n",
    "        ce = self.criterion(y_logit, y)\n",
    "        \n",
    "        pred = F.softmax(y_logit, dim=1)[:,1,:,:]\n",
    "        dice = dice_loss(pred, y)\n",
    "        \n",
    "        loss = ce * self.ce_weight + dice * (1 - self.ce_weight) * y.size(0)\n",
    "\n",
    "        return {'val_loss': loss, 'logs':{'dice':dice, 'ce': ce}}\n",
    "\n",
    "    # OPTIONAL\n",
    "    def training_epoch_end(self, outputs):\n",
    "        \"\"\"log and display average train loss and accuracy across epoch\"\"\"\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        \n",
    "        print(f\"| Train_loss: {avg_loss:.3f}\" )\n",
    "        self.log('train_loss', avg_loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "     \n",
    "    # OPTIONAL\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"log and display average val loss and accuracy\"\"\"\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        \n",
    "        avg_dice = torch.stack([x['logs']['dice'] for x in outputs]).mean()\n",
    "        avg_ce = torch.stack([x['logs']['ce'] for x in outputs]).mean()\n",
    "        \n",
    "        print(f\"[Epoch {self.trainer.current_epoch:3}] Val_loss: {avg_loss:.3f}, Val_dice: {avg_dice:.3f}, Val_ce: {avg_ce:.3f}\", end= \" \")\n",
    "        self.log_dict({'val_loss': avg_loss}, prog_bar=True, on_epoch=True, on_step=False)\n",
    "\n",
    "\n",
    "# returns trained model (also displays metrics during the training process)\n",
    "def train_model(train_data_path, pretrained=False):\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='auto',\n",
    "        max_epochs=EPOCHS_NUM\n",
    "    )\n",
    "    model = BirdSegmenter(pretrained=pretrained)\n",
    "\n",
    "    gt_folder = os.path.join(train_data_path, 'gt')\n",
    "    image_folder = os.path.join(train_data_path, 'images')\n",
    "    samples = []\n",
    "    classes = []\n",
    "    \n",
    "    # read image names and remember their classes\n",
    "    for path in Path(image_folder).rglob('*/*'):\n",
    "      samples.append( '/'.join(path.parts[-2:]) )\n",
    "      classes.append( path.parts[-2] )\n",
    "        \n",
    "    # stratified split to let the network properly learn underrepresented bird classes\n",
    "    train_idx, valid_idx = train_test_split(\n",
    "        np.arange(len(classes)),\n",
    "        test_size=TEST_SIZE,\n",
    "        stratify=classes\n",
    "    )\n",
    "\n",
    "    train_dataset = BirdDataset(np.array(samples)[train_idx], image_folder, gt_folder,\n",
    "                                transform=DEFAULT_TRAIN_TRANSFORM)\n",
    "    val_dataset = BirdDataset(np.array(samples)[valid_idx], image_folder, gt_folder,\n",
    "                             transform=DEFAULT_VAL_TRANSFORM)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# predict the segmentation mask of an imagae lying in img_path\n",
    "def predict(model, img_path):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # save as RGB and remember original shape\n",
    "        img = cv2.imread(img_path)[:,:,::-1]\n",
    "        original_shape = img.shape[:2]\n",
    "        \n",
    "        # transform image to pass through the network\n",
    "        img = DEFAULT_VAL_TRANSFORM(image=img)['image']\n",
    "        batch = torch.stack([img])\n",
    "        output = model(batch)\n",
    "        normalized_masks = torch.nn.functional.softmax(output, dim=1)\n",
    "        mask = normalized_masks[0, 1, :, :].cpu().numpy()\n",
    "\n",
    "        # restore to original shape\n",
    "        mask = A.Resize(*original_shape)(image=mask)['image']\n",
    "\n",
    "        # make the mask have the same dimensions\n",
    "        if img.shape[2] == 3:\n",
    "            mask = np.dstack([mask, mask, mask])\n",
    "        \n",
    "        return mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
