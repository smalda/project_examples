{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f472735",
   "metadata": {},
   "source": [
    "# Обучение и оценка качества детектора автомобилей\n",
    "![car_detection_frames.jpg](car_detection_frames.jpg)\n",
    "\n",
    "В данном задании реализован простой нейросетевой детектор машин на основе\n",
    "полносверточной нейросети, а также функции для подсчета\n",
    "метрики качества и подавления множественных обнаружений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbde46",
   "metadata": {},
   "source": [
    "Fully convolutional классификатор `Classifier` обучается на изображениях машин (40, 100, 1), затем от его выхода берётся значение Softmax для класса, соответствующего обнаружжению машины. Такой детектор `DetectorFromClassifier` можно применять к любым изображениям больше исходного (по сути свёртка с ядром (40, 100)). В пикселе полученного изображения будет уверенность детектора в том, что машина обнаружена в рамке размера (40, 100), для которой пиксель - верхний левый угол."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf0a07",
   "metadata": {},
   "source": [
    "Реализованы следующие методы:  \n",
    "`get_detections`: применяет детектор к набору изображений и возвращает в формате bbox все обнаружения с *confidence* больше фискированного порога  \n",
    "`calc_iou`: подсчитывает *IoU* между двумя bbox  \n",
    "`calc_auc`: по предсказаниям и gt bboxes подсчитывает PR AUC (вариация по confidence threshold)  \n",
    "`nms`: реализует простую версию алгоритма подавления немаксимумов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cf4c9",
   "metadata": {},
   "source": [
    "Алгоритм `calc_auc`:\n",
    "1. Для каждого изображения составляется список из tp (true positive) и fp (false positive)\n",
    "обнаружений. Для этого:  \n",
    "(a) Сортируются обнаружения в порядке убывания соответствующих мер уверенности классификатора. Подготавливается список gt прямоугольников из разметки данного изображения.  \n",
    "(b) Для каждого обнаружения находится соответствующий ему прямоугольник из разметки, для\n",
    "которого мера IoU максимальна и > iou_thr.  \n",
    "(c) Если такой прямоугольник найден, то обнаружение добавляется в tp, иначе — в fp.  \n",
    "(d) Чтобы не сопоставлять один и тот же прямоугольник из разметки двум обнаружениями\n",
    "детектора, после добавления обнаружения в tp соответствующий ему прямоугольник\n",
    "удаляется из gt.  \n",
    "2. Объединяются списки tp и fp всех изображений.  \n",
    "3. Теперь нужны два списка — все обнаружения pp (объединение tp и fp) и tp. \n",
    "Эти списки сортируются по возрастанию мер уверенности классификатора.  \n",
    "4. Проход по списку всех обнаружений: пусть сейчас рассматривается обнаружение\n",
    "с мерой уверенности c, тогда подсчитывается количество всех обнаружений с мерой уверенности *> c* и\n",
    "количество tp обнаружений с уверенностью *> c*.  \n",
    "5. Из полученных данных и общего количество прямоугольников в разметке рассчитываются recall\n",
    "и precision для каждого порога уверенности c.  \n",
    "6. Собираются полученные тройки (recall, precision, c), задающие PR кривую, и подсчитывается AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346bbe9",
   "metadata": {},
   "source": [
    "Алгоритм `nms`:\n",
    "1. Обнаружения сортируются по убыванию меры уверенности.  \n",
    "2. Для каждого обнаружения удаляются все следующие за ним обнаружения (те, у которых мера\n",
    "уверенности меньше), которые пересекаются с данным по мере IoU больше, чем на iou_thr."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23151c53",
   "metadata": {},
   "source": [
    "Все `bbox` в формате `[row, col, n_rows, n_cols (, confidence)]`, где `row` - строка, с которой начинается `bbox`, `col` - соответствующая колонка, `n_row` и `n_col` - количество строк и колонок, занимаемых `bbox`, `confidence` - (опционально) уверенность детектора в том, что в данной рамке находится машина. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, auc\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 20\n",
    "\n",
    "train_transform = nn.Sequential(*[\n",
    "    transforms.Normalize((0.508), (0.291)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    transforms.RandomAdjustSharpness(2),\n",
    "]).to(device)\n",
    "\n",
    "test_transform = transforms.Normalize((0.508), (0.291)).to(device)\n",
    "\n",
    "\n",
    "# ============================== 1 Classifier model ============================\n",
    "# simple FCN classifier to check if there's a car on the image\n",
    "# outputs logits\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier_, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(10):\n",
    "            self.layers.extend([\n",
    "                nn.Conv2d( 2**((i+1)//2), 2**((i+2)//2), 3),\n",
    "                nn.BatchNorm2d(2**((i+2)//2)),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "        \n",
    "        self.layers.extend([\n",
    "            nn.Conv2d(32, 128, (20, 80)),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 2, 1),\n",
    "        ])\n",
    "        self.process = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.process(x)\n",
    "\n",
    "\n",
    "# train Classifier on available data\n",
    "# (simple procedure without validation as this isn't the aim of the task)\n",
    "def fit_cls_model(X, y):\n",
    "    \"\"\"\n",
    "    :param X: 4-dim tensor with training images\n",
    "    :param y: 1-dim tensor with labels for training\n",
    "    :return: trained nn model\n",
    "    \"\"\"\n",
    "    model = Classifier().to(device)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # return best model according to accuracy on train dataset\n",
    "    best_model = None\n",
    "    history = {'test_acc': []}\n",
    "    \n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        model.train()\n",
    "        test_acc = []\n",
    "        for sample in dataloader:\n",
    "            X_batch, y_batch = sample\n",
    "            X_batch = train_transform(X_batch.to(device))\n",
    "            y_batch = y_batch.to(device)\n",
    "            output = model(X_batch).reshape(-1, 2)\n",
    "\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            preds = torch.argmax(F.softmax(output.detach(), dim=1), dim=1)\n",
    "            test_acc.append(accuracy_score(y_batch.cpu().numpy(), preds.cpu().numpy()))\n",
    "\n",
    "        history['test_acc'].append(np.mean(test_acc))\n",
    "\n",
    "        if np.argmax(history['test_acc']) == epoch:\n",
    "            best_model = model\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# ============================ 2 Classifier -> Detector =============================\n",
    "# detection wrapper for classifier to return only logits corresponding to the class 'car',\n",
    "# normalized with Softmax\n",
    "class DetectorFromClassifier(nn.Module):\n",
    "    def __init__(self, cls_model):\n",
    "        super(DetectorFromClassifier, self).__init__()\n",
    "        self.base_model = cls_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)[:,1,:,:]\n",
    "    \n",
    "def get_detection_model(cls_model):\n",
    "    \"\"\"\n",
    "    :param cls_model: trained cls model\n",
    "    :return: fully convolutional nn model with weights initialized from cls\n",
    "             model\n",
    "    \"\"\"\n",
    "    detection_model = DetectorFromClassifier(nn.Sequential(test_transform, cls_model, nn.Softmax(dim=1)))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================ 3 Simple detector ===============================\n",
    "# returns all bboxes where detector's confidence is higher than conf_thru\n",
    "def get_detections(detection_model, dictionary_of_images, conf_thru=0.7):\n",
    "    \"\"\"\n",
    "    :param detection_model: trained fully convolutional detector model\n",
    "    :param dictionary_of_images: dictionary of images in format\n",
    "        {filename: ndarray}\n",
    "    :return: detections in format {filename: detections}. detections is a N x 5\n",
    "        array, where N is number of detections. Each detection is described\n",
    "        using 5 numbers: [row, col, n_rows, n_cols, confidence].\n",
    "    \"\"\"\n",
    "\n",
    "    detection_model = detection_model.to(device)\n",
    "    detections = {}\n",
    "    with torch.inference_mode():\n",
    "        for filename, image in tqdm(dictionary_of_images.items()):\n",
    "            heatmap = detection_model(torch.from_numpy(image)[None, None, ...].to(device)).cpu().numpy()[0]\n",
    "            rows, cols = np.where(heatmap > conf_thru)\n",
    "            detections[filename] = np.dstack([\n",
    "                rows, cols, np.full_like(rows, 40), np.full_like(rows, 100), heatmap[rows, cols]\n",
    "            ])[0]\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "# =============================== 5 IoU ========================================\n",
    "# calculates IoU of 2 bboxes\n",
    "def calc_iou(first_bbox, second_bbox):\n",
    "    \"\"\"\n",
    "    :param first bbox: bbox in format (row, col, n_rows, n_cols)\n",
    "    :param second_bbox: bbox in format (row, col, n_rows, n_cols)\n",
    "    :return: iou measure for two given bboxes\n",
    "    \"\"\"\n",
    "    area1 = first_bbox[2] * first_bbox[3]\n",
    "    area2 = second_bbox[2] * second_bbox[3]\n",
    "\n",
    "    left = max(first_bbox[1], second_bbox[1])\n",
    "    right = min(first_bbox[1] + first_bbox[3], second_bbox[1] + second_bbox[3])\n",
    "    top = max(first_bbox[0], second_bbox[0])\n",
    "    bottom = min(first_bbox[0] + first_bbox[2], second_bbox[0] + second_bbox[2])\n",
    "\n",
    "    intersection_area = max(0, right - left) * max(0, bottom - top)\n",
    "\n",
    "    return intersection_area / float(area1 + area2 - intersection_area)\n",
    "\n",
    "\n",
    "\n",
    "# =============================== 6 AUC ========================================\n",
    "# calculates PR AUC for given predicted and gt bboxes\n",
    "def calc_auc(pred_bboxes, gt_bboxes):\n",
    "    \"\"\"\n",
    "    :param pred_bboxes: dict of bboxes in format {filename: detections}\n",
    "        detections is a N x 5 array, where N is number of detections. Each\n",
    "        detection is described using 5 numbers: [row, col, n_rows, n_cols,\n",
    "        confidence].\n",
    "    :param gt_bboxes: dict of bboxes in format {filenames: bboxes}. bboxes is a\n",
    "        list of tuples in format (row, col, n_rows, n_cols)\n",
    "    :return: auc measure for given detections and gt\n",
    "    \"\"\"\n",
    "    \n",
    "    # only bboxes whose IoU with gt is >=IOU_THR are considered \n",
    "    IOU_THR = 0.5\n",
    "\n",
    "    # tp - pred_bboxes matched with some gt\n",
    "    # fp - pred_bboxes that are either worse than matches from TP or don't have any match with gt at all\n",
    "    tp, fp = [], []\n",
    "    # number of all positives (sum of sizes of gt) for PR calculations\n",
    "    number_p = 0\n",
    "\n",
    "    # iterate over each picture\n",
    "    for filename, detections in pred_bboxes.items():\n",
    "        # sort detections with confidence descending\n",
    "        detections = detections[np.argsort(detections[:, -1])][::-1]\n",
    "        gt = np.array(gt_bboxes[filename])\n",
    "        \n",
    "        # discovered gt bboxes\n",
    "        inspected = set()\n",
    "        # inspect detections with higher confidence first\n",
    "        for detection in detections:\n",
    "            iou_best = 0\n",
    "            idx_best = None\n",
    "            \n",
    "            # find the best match among gt according to IoU\n",
    "            for i in range(len(gt)):\n",
    "                if i in inspected:\n",
    "                    continue\n",
    "                else:\n",
    "                    iou = calc_iou(detection[:4], gt[i])\n",
    "                    if iou >= IOU_THR and iou > iou_best:\n",
    "                        idx_best = i\n",
    "                        iou_best = iou\n",
    "            \n",
    "            # found new tp\n",
    "            if iou_best >= IOU_THR:\n",
    "                inspected.add(idx_best)\n",
    "                tp.append(detection)\n",
    "            else:\n",
    "                fp.append(detection)\n",
    "                \n",
    "        number_p += len(gt)\n",
    "    \n",
    "\n",
    "    # sort all TP and PP (predicted positives) detections with confidence ascending\n",
    "    tp = np.array(tp)\n",
    "    tp = tp[np.argsort(tp[:, -1])]\n",
    "    if fp:\n",
    "        pp = np.concatenate([tp, fp])\n",
    "        pp = p[np.argsort(p[:, -1])]\n",
    "    else:\n",
    "        pp = tp\n",
    "\n",
    "    N = len(pp)\n",
    "    pr_curve = [(0,1)]\n",
    "    \n",
    "    # we'll iterate over all confidence thresholds in PP bboxes\n",
    "    # given c^, we assume the detector decides bboxes with conf>=c^ are true\n",
    "    old_c = 0\n",
    "    for i in range(N):\n",
    "        c = pp[i, -1]\n",
    "        \n",
    "        # if we reach a new conf threshold, we'll calculate new P and R\n",
    "        if old_c == c:\n",
    "            continue\n",
    "        else:\n",
    "            old_c = c\n",
    "        \n",
    "        # Npp = number of predicted positives if conf>=c^\n",
    "        # Ntp = -----     true      ---\n",
    "        Npp = N - i\n",
    "        Ntp = (tp[:, -1] >= c).sum()\n",
    "        # append new (recall, precision)\n",
    "        # number_p = number of all gt bboxes\n",
    "        pr_curve.append( (Ntp / number_p, Ntp / Npp) )\n",
    "\n",
    "    points = np.array(pr_curve)\n",
    "    points = points[np.argsort(points[:, 0])]\n",
    "    points = points[np.lexsort([1 - points[:,1], points[:,0]])]\n",
    "\n",
    "    return auc(points[:,0], points[:,1])\n",
    "\n",
    "\n",
    "# =============================== 7 NMS ========================================\n",
    "# use Non-maximum Suppression algorithm to get rid of detections thata are too close to each other\n",
    "def nms(detections_dictionary, iou_thr=0.05):\n",
    "    \"\"\"\n",
    "    :param detections_dictionary: dict of bboxes in format {filename: detections}\n",
    "        detections is a N x 5 array, where N is number of detections. Each\n",
    "        detection is described using 5 numbers: [row, col, n_rows, n_cols,\n",
    "        confidence].\n",
    "    :param iou_thr: IoU threshold for nearby detections\n",
    "    :return: dict in same format as detections_dictionary where close detections\n",
    "        are deleted\n",
    "    \"\"\"\n",
    "    processed_detections = {}\n",
    "    # iterate over each image and its bboxes\n",
    "    for filename, detections in tqdm(detections_dictionary.items()):\n",
    "        detections = np.array(detections)\n",
    "        \n",
    "        # we'll iterate over detections with confidence descending\n",
    "        detections = detections[np.argsort(detections[:, -1])][::-1]\n",
    "        # set of rejected detections\n",
    "        deleted = set()\n",
    "        for i in range(len(detections)):\n",
    "            if i not in deleted:\n",
    "                for j in range(i + 1, len(detections)):\n",
    "                    if j not in deleted and calc_iou(detections[i,:4], detections[j,:4]) > iou_thr:\n",
    "                        deleted.add(j)\n",
    "\n",
    "        # get indices of all accepted detections\n",
    "        mask = list(set(range(len(detections))) - deleted)\n",
    "        processed_detections[filename] = detections[mask]\n",
    "    \n",
    "    return processed_detections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
